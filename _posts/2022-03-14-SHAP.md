---
layout: post
toc: true
title: "SHAP 논문 정리"
categories: XAI
tags: [SHAP, XAI]
---

# A Unified Approach to Interpreting Model Predictions
- Author: Scoot M. Lundberg, Su-In Lee
- Abstract
  - 모델이 어떤 결정을 내린 이유를 이해하는 것은 많은 application에서 예측 정확도 측면에서 중요
  - 현대의 큰 데이터 셋에는 주로 Complex model(Deep learning, ensemble)을 사용하여 높은 정확도를 얻음
    - complex model은 interpretability와 complex 사이에서 trade-off가 존재
  - 따라서 이러한 모델들을 해석하려는 시도가 존재했으나, 이 방법들이 어떻게 연관이 있는지, 어떤 상황에서 어떤 방법을 써야 하는 지에 대해서는 아직 불분명함
  - 이 논문에서는 U특정 예측에 대한 각각의 feature가 가지는 중요도를 계산하여 제시하는 Unified framework SHAP을 제안

## 1. Introduction

다음과 같은 이유로 예측 모델의 output을 해석하려는 것은 매우 중요함
``` shell
  - 사용자의 신뢰 형성
  - 모델을 개선할 수 있는 방법에 대한 통찰
  - 모델링중인 프로세스에 대한 이해
```
해석력 때문에 complex 모델이 더 성능이 좋은 경우라도 해석이 쉬운 모델을 사용하는 경우가 존재하나, 근래의 빅데이터의 효용성이 올라가면서 complex model을 사용했을 때 얻는 이점이 점점 증가하여 interpretability와 complex 사이의 trade-off가 새로운 문제로 대두됨

이 문제를 해결하기위해 다양한 방법들이 제안되었으나, 이 방법들이 어떻게 연관이 되어있고, 어떤 상황에서 어떤 방법을 써야하는 지에 대한 이해가 부족한 상황임

이 논문에서 제시하는 SHAP framework는 다음과 같은 결과로 이어짐
```shell
  1. explanation model이라 부르는 모델의 예측에 대한 설명을 모델 자체로 보는 관점을 소개
    - additive feature attribution method를 정의
  2. 고유한 솔루션을 보장하는 게임이론 결과가 additive feature attribution method의 모든 class에 적용됨을 보이고, 다양한 방법에 근사할 수 있는 feature importance의 통합 척도로 SHAP value를 제안
  3. ~ 
```

## 2. Additive Feature Attribution Methods
Simple 모델의 가장 좋은 설명은 모델 그 자체로, 설명이 모델 자체를 완벽하게 표현할 수 있고 이해하기도 쉬움
앙상블, deep learning 같은 complex model은 이해하기 어렵기 때문에 원래 모델을 설명으로 사용하기 어려워 원래 모델의 해석 가능한 근사치로 정의되는 설명 모델을 사용

``` shell
f: 설명할 original model
g: 설명 model
```
LIME에서는 single input x에 대한 prediction(x)를 local method로 설명하고 있음
  - 설명 모델은 매핑 함수 $X=h_x(x')$를 통해 원래 입력에 매핑되는 단순한 입력 x'를 사용
  - local 방법은 $g(z') ~ f(h_x(z'))$을 보장하려고 함
  - $h_x$는 x에 대해 고유하기 때문에 x'은 x보다 적은 정보를 가질 수 있음

```
# Def 1. Additive feature attribution method는 binary variable의 선형함수의 형태로 설명 모델을 가짐

$g(z') = \phi_0 + \sumation \phi_i z_i'$  -- Equation (1)
  - z' \in {0, 1}^M
  - \phi_i \in R
```
정의 1의 설명 모델을 가지는 방법은 $\phi_i$(각 feature의 효과)를 속성으로 가지며 모든 효과를 합치면 원래 모델의 output f(x)와 근사하게 됨


### 2.1 LIME
주어진 예측 주변에서 모델을 근사화하여 개별 모델 예측을 해석하는 방법
LIME의 local 선형 설명모델은 Equation (1)을 따르기 때문에 additive feature attribution method라고 할 수 있음
LIME에서는 x'을 해석가능한 input으로 부르며, $x=h_x(x'))$는 해석가능한 input의 이진 벡터를 원래 input space로 변환
```
  - text: $h_x$는 원래 단어가 표현되느냐 안 되느냐에 따라 1 또는 0으로 변환(단어가 존재 -> 1, 존재하지 않음 -> 0)
  - image: $h_x$는 이미지를 super pixels의 set으로 처리하여 super pixel을 그대로 나두면 1, 주변 pixel의 평균으로 대체(missing)하면 0으로 변환
``` 

$\phi$를 찾기 위해 LIME은 아래의 objective function을 최소화
```
$\ohm = argmin_{g\inG} L(f, g, \pi_{x'}) + \ohm(g)$
  - $\pi_{x'}$: local kernel
  - $\ohm(g)$: 복잡도(penalty)
```

### 2.2 Deep LIFT
딥러닝을 위한 재귀적 예측 설명 방법
각 입력값의 $C_{\delta x_i}_{\delta y}$는 원래 input 값과 반대되는 reference 값으로 설정된 input의 효과를 의미함
$x = h_x(x')$는 다음과 같이 binary로 변환
```
  - 1: 원래의 value
  - 0: reference value
```
유저가 선택한 reference value는 feature에 대한 일반적인 정보가 없는 background value
Deep LIFT는 summation-to-delta라는 속성을 사용
```
$\sumation_{i=1} C_{\delta x_i}_{\delta o} = \delta o $
  - o = f(x): model output
  - $\delta o = f(x) - f(r)$
  - $\delta x_i = x_i - r_i$
  - r: reference input
```
만약 $\phi_i = C_{\delta x_i}_{\delta o}$ 그리고 $\phi_0 = f(r)$ 이라면, DEEPLIFT의 설명 모델은 Equation 1과 매칭되기 때문에 이 방법 역시 additive feature attribution으로 생각할 수 있음



### 4. Discussion
stacked LSTM net이 패턴 지속 시간에 대한 사전 지식 없이 더 높은 수준의 시간 패턴을 학습할 수 있고 따라서 정상적인 시계열 동작을 모델링 하여 이상을 탐지할 수 있음을 보임
LSTM-AD 접근 방식은 단기 및 장기 시간 종속성을 모델링하는 4개의 실제 데이터 세트에서 유망한 결과를 산출
RNN-AD와 비교하여 더 좋거나 유사한 결과를 제공하여 RNN 기반 모델에 비해 더 강력할 수 있음을 보이고, 특히 정상적인 동작이 장기 종속성을 포함하는 지 여부를 미리 알 수 없는 경우에 더 강력함
